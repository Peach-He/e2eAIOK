{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b92539e",
   "metadata": {},
   "source": [
    "# AIOK Model Adapter Distiller Customized DEMO\n",
    "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
    "\n",
    "This demo mainly introduces the usage of Distiller. Take image classification as an example, it shows how to integrate distiller  from ResNet50 to ResNet18 on CIFAR100 dataset. This demo shows how to integrate distiller into a general training pipeline, you can find build-in simplied demo at [here](./Model_Adapter_Distiller_buildin_ResNet18_CIFAR100.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef213f",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* [Model Adapter Distiller Overview](#Model-Adapter-Distller-Overview)\n",
    "* [Environment Setup](#Environment-Setup)\n",
    "* [Training with Distiller](#Training-with-Distiller)\n",
    "    * [Prepare Data](#Prepare-Data)\n",
    "    * [Create Transferrable Model](#Create-Transferrable-Model)\n",
    "    * [Train and Evaluate](#Train-and-Evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6cb09",
   "metadata": {},
   "source": [
    "# Model Adapter Distiller Overview\n",
    "Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "\n",
    "<img src=\"../imgs/distiller.png\" width=\"60%\">\n",
    "<center>Model Adapter Distiller Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2fc58",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "1. prepare code\n",
    "    ``` bash\n",
    "    git clone https://github.com/intel/e2eAIOK.git\n",
    "    cd e2eAIOK\n",
    "    git submodule update --init –recursive\n",
    "    ```\n",
    "2. build docker image\n",
    "   ```\n",
    "   cd Dockerfile-ubuntu18.04 && docker build -t e2eaiok-pytorch112 . -f DockerfilePytorch112 && cd .. && yes \n",
    "   ```\n",
    "3. run docker\n",
    "   ``` bash\n",
    "   docker run -it --name model_adapter --shm-size=10g --privileged --network host \\\n",
    "   -v ${dataset_path}:/home/vmagent/app/data  \\\n",
    "   -v `pwd`:/home/vmagent/app/e2eaiok \\\n",
    "   -w /home/vmagent/app/e2eaiok e2eaiok-pytorch112 /bin/bash \n",
    "   ```\n",
    "4. Run in conda and set up e2eAIOK\n",
    "   ```bash\n",
    "   conda activate pytorch-1.12.0\n",
    "   python setup.py sdist && pip install dist/e2eAIOK-*.*.*.tar.gz\n",
    "   ```\n",
    "5. Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636494a",
   "metadata": {},
   "source": [
    "# Training with Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d8e21",
   "metadata": {},
   "source": [
    "Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f878badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from timm.utils import accuracy\n",
    "import timm\n",
    "import transformers\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28cbf4",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893b32d",
   "metadata": {},
   "source": [
    "### Define Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c016312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343) # mean for 3 channels\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)  # std for 3 channels\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.Resize(112),  # pretrained model is trained on large imgage size, scale 32x32 to 112x112\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.Resize(112),  # pretrained model is trained on large imgage size, scale 32x32 to 112x112\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fb033",
   "metadata": {},
   "source": [
    "### Prepare dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69896736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_folder='/home/vmagent/app/data/dataset/cifar' # dataset location\n",
    "train_set = datasets.CIFAR100(root=data_folder, train=True, download=True, transform=train_transform)\n",
    "test_set = datasets.CIFAR100(root=data_folder, train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=128, shuffle=True, num_workers=1, drop_last=False)\n",
    "validate_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeba42",
   "metadata": {},
   "source": [
    "## Create Transferrable Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ea401",
   "metadata": {},
   "source": [
    "### Create Backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b8a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model('resnet18', pretrained=False, num_classes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b6c52",
   "metadata": {},
   "source": [
    "### Create teacher model\n",
    "To use distiller, we need to prepare teacher model to guide the training. Directly download teacher model Resnet50 pretrained on CIFAR100 from [here](), and put it at `${dataset}/model/demo/baseline/cifar100_res50PretrainI21k/cifar100_res50_pretrain_imagenet21k.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3d1996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model = \"/home/vmagent/app/data/model/demo/baseline/cifar100_res50PretrainI21k/cifar100_res50_pretrain_imagenet21k.pth\"\n",
    "teacher_model = timm.create_model('resnet50', pretrained=False, num_classes=100)\n",
    "teacher_model.load_state_dict(torch.load(pretrain_model), strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353b6d3",
   "metadata": {},
   "source": [
    "### Define Distiller\n",
    "Here we define a distiller using KD algorithm, and it take a teacher model as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c86c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2eAIOK.ModelAdapter.engine_core.distiller import KD\n",
    "distiller= KD(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1bd41",
   "metadata": {},
   "source": [
    "### Make Model transferrable with distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1b2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2eAIOK.ModelAdapter.engine_core.transferrable_model import *\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model = make_transferrable_with_knowledge_distillation(backbone,loss_fn,distiller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e633199",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa226f93",
   "metadata": {},
   "source": [
    "### Create optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9561a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# create optimizer #################\n",
    "init_lr = 0.01\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(),lr=init_lr, weight_decay=weight_decay,momentum=momentum)\n",
    "################# create scheduler #################\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a0628",
   "metadata": {},
   "source": [
    "### Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046525dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 1 # max 1 epoch\n",
    "print_interval = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "874fb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        \n",
    "    def train(self, train_dataloader, valid_dataloader, max_epoch):\n",
    "        ''' \n",
    "        :param train_dataloader: train dataloader\n",
    "        :param valid_dataloader: validation dataloader\n",
    "        :param max_epoch: steps per epoch\n",
    "        '''\n",
    "        for epoch in range(0, max_epoch):\n",
    "            ################## train #####################\n",
    "            model.train()  # set training flag\n",
    "            for (cur_step,(data, label)) in enumerate(train_dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss_value = model.loss(output, label) # transferrable model has loss attribute\n",
    "                loss_value.backward() \n",
    "                if cur_step%print_interval == 0:\n",
    "                    batch_acc = accuracy(output.backbone_output,label)[0] # use output.backbone_output instead of output\n",
    "                    dt = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # date time\n",
    "                    print(\"[{}] epoch {} step {} : training backbone loss {:.4f}, distiller loss {:4f}, training batch acc {:.4f}\".format(\n",
    "                      dt, epoch, cur_step, loss_value.backbone_loss.item(), loss_value.distiller_loss.item(), batch_acc.item())) # use loss_value.backbone_loss\n",
    "                self._optimizer.step()\n",
    "            self._scheduler.step()\n",
    "            ################## evaluate ######################\n",
    "            self.evaluate(model, valid_dataloader, epoch)\n",
    "            \n",
    "    def evaluate(self, model, valid_dataloader, epoch):\n",
    "        with torch.no_grad():\n",
    "            model.eval()  \n",
    "            backbone = model.backbone # use backbone in evaluation\n",
    "            loss_cum = 0.0\n",
    "            sample_num = 0\n",
    "            acc_cum = 0.0\n",
    "            for (cur_step,(data, label)) in enumerate(valid_dataloader):\n",
    "                output = backbone(data)\n",
    "                batch_size = data.size(0)\n",
    "                sample_num += batch_size\n",
    "                loss_cum += loss_fn(output, label).item() * batch_size\n",
    "                acc_cum += accuracy(output, label)[0].item() * batch_size\n",
    "            dt = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # date time\n",
    "            loss_value = loss_cum/sample_num\n",
    "            acc_value = acc_cum/sample_num\n",
    "\n",
    "            print(\"[{}] epoch {} : evaluation loss {:.4f}, evaluation acc {:.4f}\".format(\n",
    "                dt, epoch, loss_value, acc_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc734272",
   "metadata": {},
   "source": [
    "### Start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae383b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-07 09:32:04] epoch 0 step 0 : training backbone loss 4.2446, distiller loss 7.003967, training batch acc 7.0312\n",
      "[2023-02-07 09:32:21] epoch 0 step 10 : training backbone loss 4.1627, distiller loss 6.882109, training batch acc 8.5938\n",
      "[2023-02-07 09:32:36] epoch 0 step 20 : training backbone loss 3.9140, distiller loss 6.864616, training batch acc 9.3750\n",
      "[2023-02-07 09:32:51] epoch 0 step 30 : training backbone loss 3.8867, distiller loss 6.332502, training batch acc 11.7188\n",
      "[2023-02-07 09:33:07] epoch 0 step 40 : training backbone loss 3.9411, distiller loss 7.078649, training batch acc 10.1562\n",
      "[2023-02-07 09:33:22] epoch 0 step 50 : training backbone loss 4.2199, distiller loss 6.615680, training batch acc 8.5938\n",
      "[2023-02-07 09:33:37] epoch 0 step 60 : training backbone loss 4.3427, distiller loss 6.883377, training batch acc 5.4688\n",
      "[2023-02-07 09:33:52] epoch 0 step 70 : training backbone loss 4.1152, distiller loss 6.332665, training batch acc 10.1562\n",
      "[2023-02-07 09:34:06] epoch 0 step 80 : training backbone loss 4.0949, distiller loss 7.098349, training batch acc 9.3750\n",
      "[2023-02-07 09:34:22] epoch 0 step 90 : training backbone loss 3.9931, distiller loss 7.163930, training batch acc 7.8125\n",
      "[2023-02-07 09:34:37] epoch 0 step 100 : training backbone loss 3.9267, distiller loss 7.757466, training batch acc 11.7188\n",
      "[2023-02-07 09:34:52] epoch 0 step 110 : training backbone loss 3.9695, distiller loss 6.348218, training batch acc 11.7188\n",
      "[2023-02-07 09:35:07] epoch 0 step 120 : training backbone loss 3.8754, distiller loss 7.408428, training batch acc 10.1562\n",
      "[2023-02-07 09:35:22] epoch 0 step 130 : training backbone loss 4.3683, distiller loss 6.997625, training batch acc 3.9062\n",
      "[2023-02-07 09:35:38] epoch 0 step 140 : training backbone loss 4.0281, distiller loss 6.790541, training batch acc 7.0312\n",
      "[2023-02-07 09:35:53] epoch 0 step 150 : training backbone loss 3.9315, distiller loss 6.082658, training batch acc 7.8125\n",
      "[2023-02-07 09:36:08] epoch 0 step 160 : training backbone loss 3.8787, distiller loss 6.955656, training batch acc 14.0625\n",
      "[2023-02-07 09:36:24] epoch 0 step 170 : training backbone loss 3.9656, distiller loss 6.936814, training batch acc 8.5938\n",
      "[2023-02-07 09:36:39] epoch 0 step 180 : training backbone loss 4.0684, distiller loss 6.984304, training batch acc 5.4688\n",
      "[2023-02-07 09:36:54] epoch 0 step 190 : training backbone loss 4.1230, distiller loss 7.302319, training batch acc 5.4688\n",
      "[2023-02-07 09:37:09] epoch 0 step 200 : training backbone loss 4.0783, distiller loss 7.023134, training batch acc 7.0312\n",
      "[2023-02-07 09:37:24] epoch 0 step 210 : training backbone loss 4.0009, distiller loss 7.611389, training batch acc 10.1562\n",
      "[2023-02-07 09:37:39] epoch 0 step 220 : training backbone loss 3.9685, distiller loss 6.805190, training batch acc 11.7188\n",
      "[2023-02-07 09:37:54] epoch 0 step 230 : training backbone loss 4.0074, distiller loss 6.757984, training batch acc 6.2500\n",
      "[2023-02-07 09:38:08] epoch 0 step 240 : training backbone loss 4.0428, distiller loss 6.688333, training batch acc 8.5938\n",
      "[2023-02-07 09:38:23] epoch 0 step 250 : training backbone loss 3.9379, distiller loss 6.526119, training batch acc 7.0312\n",
      "[2023-02-07 09:38:39] epoch 0 step 260 : training backbone loss 3.9295, distiller loss 6.703921, training batch acc 13.2812\n",
      "[2023-02-07 09:38:53] epoch 0 step 270 : training backbone loss 4.2905, distiller loss 6.667858, training batch acc 3.1250\n",
      "[2023-02-07 09:39:08] epoch 0 step 280 : training backbone loss 3.9735, distiller loss 6.398967, training batch acc 9.3750\n",
      "[2023-02-07 09:39:23] epoch 0 step 290 : training backbone loss 4.0870, distiller loss 6.114409, training batch acc 4.6875\n",
      "[2023-02-07 09:39:38] epoch 0 step 300 : training backbone loss 4.1412, distiller loss 7.095528, training batch acc 5.4688\n",
      "[2023-02-07 09:39:54] epoch 0 step 310 : training backbone loss 4.0101, distiller loss 6.786016, training batch acc 11.7188\n",
      "[2023-02-07 09:40:08] epoch 0 step 320 : training backbone loss 4.2560, distiller loss 7.241052, training batch acc 3.1250\n",
      "[2023-02-07 09:40:24] epoch 0 step 330 : training backbone loss 3.9432, distiller loss 6.644112, training batch acc 6.2500\n",
      "[2023-02-07 09:40:40] epoch 0 step 340 : training backbone loss 3.8797, distiller loss 6.380994, training batch acc 9.3750\n",
      "[2023-02-07 09:40:55] epoch 0 step 350 : training backbone loss 3.9471, distiller loss 7.316897, training batch acc 9.3750\n",
      "[2023-02-07 09:41:11] epoch 0 step 360 : training backbone loss 3.8967, distiller loss 6.921604, training batch acc 10.9375\n",
      "[2023-02-07 09:41:26] epoch 0 step 370 : training backbone loss 4.0719, distiller loss 6.966424, training batch acc 7.8125\n",
      "[2023-02-07 09:41:42] epoch 0 step 380 : training backbone loss 3.9359, distiller loss 6.653729, training batch acc 8.5938\n",
      "[2023-02-07 09:41:57] epoch 0 step 390 : training backbone loss 3.9459, distiller loss 7.247851, training batch acc 12.5000\n",
      "[2023-02-07 09:42:07] epoch 0 : evaluation loss 3.9899, evaluation acc 9.1600\n",
      "CPU times: user 6h 33min 21s, sys: 1h 2min 42s, total: 7h 36min 3s\n",
      "Wall time: 10min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(model, optimizer, scheduler)\n",
    "trainer.train(train_loader,validate_loader,max_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
