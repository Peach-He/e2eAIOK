{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417eb0d3",
   "metadata": {},
   "source": [
    "# Save logits from VIT with Distiller on CIFAR100\n",
    "Distiller can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure.\n",
    "\n",
    "* Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacherâ€™s knowledge.\n",
    "* Distiller can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge  speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "![Distiller](../doc/imgs/distiller.png)\n",
    "\n",
    "However, during the distillation process, teacher forwarding usually takes a lot of time. We can use logits saving function in distiller to save predictions from teacher in adavance, then lots of time can be saved during student training.\n",
    "\n",
    "## Notebook Content\n",
    "In this notebook, we will show how to train the model with logits saved before, and here we still take ResNet18 from VIT as an example.\n",
    "\n",
    "To use logits saved before for backbone training, we just need to update three steps:\n",
    "- Wrap train_dataset with DataWrapper, but set save_logits to False\n",
    "- When define Distiller, set use_saved_logits to be True\n",
    "- When epoch changes, call dataset.set_epoch(epoch)\n",
    "\n",
    "Note: Data preprocessor for student and teacher can be different, but for all the process with random augmentation, they must keep same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008eaf36",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f878badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import transformers\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc00450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/vmagent/app/TLK/frameworks.bigdata.AIDK/AIDK/\")\n",
    "from TransferLearningKit.src.engine_core.transferrable_model import make_transferrable_with_knowledge_distillation\n",
    "from TransferLearningKit.src.engine_core.distiller import KD\n",
    "from TransferLearningKit.src.engine_core.distiller.utils import logits_wrap_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd339d",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "### Define Data Preprocessor for student\n",
    "For student, we can use original image size 32x32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77065ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343) # mean for 3 channels\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)  # std for 3 channels\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293597d0",
   "metadata": {},
   "source": [
    "### Prepare and warp dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49f273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_workers = 1 # data worker\n",
    "data_folder='./dataset' # dataset location\n",
    "train_set = datasets.CIFAR100(root=data_folder, train=True, download=True, transform=train_transform)\n",
    "test_set = datasets.CIFAR100(root=data_folder, train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdeee4e",
   "metadata": {},
   "source": [
    "Warp train dataset with DataWrapper, but set save_logits flag to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b442080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_path = './logits'\n",
    "save_logits = False # save logits\n",
    "train_set = logits_wrap_dataset(train_set, logits_path=logits_path, num_classes=100, save_logits=save_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4703b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=False)\n",
    "validate_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeba42",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660678e1",
   "metadata": {},
   "source": [
    "### Create Backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b8a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = timm.create_model('resnet18', pretrained=False, num_classes=100).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ac27b",
   "metadata": {},
   "source": [
    "### Define Distiller \n",
    "When define distiller, we need to define teacher_type with a name start with \"huggingface\" if the teacher model comes from hugging face. Otherwise, don't need to set it. \n",
    "\n",
    "Set use_saved_logits to be True when we want to load logits saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d759945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.87 s, sys: 353 ms, total: 3.22 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "teacher_model = transformers.ViTForImageClassification.from_pretrained('edumunozsala/vit_base-224-in21k-ft-cifar100')\n",
    "distiller= KD(teacher_model,teacher_type=\"huggingface_vit_base-224-in21k-ft-cifar100\", use_saved_logits=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6e4c5",
   "metadata": {},
   "source": [
    "### Make Model transferrable with distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6456289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_transferrable_with_knowledge_distillation(model, loss_fn,distiller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa226f93",
   "metadata": {},
   "source": [
    "# create optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9561a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# create optimizer #################\n",
    "init_lr = 0.01\n",
    "weight_decay = 0.005\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(),lr=init_lr, weight_decay=weight_decay,momentum=momentum)\n",
    "################# create scheduler #################\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e633199",
   "metadata": {},
   "source": [
    "# Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4f73dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 1 # max 1 epoch\n",
    "print_interval = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559b2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output,label):\n",
    "    pred = output.data.cpu().max(1)[1]\n",
    "    label = label.data.cpu()\n",
    "    if label.shape == output.shape:\n",
    "        label = label.max(1)[1]\n",
    "    return torch.mean((pred == label).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874fb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        \n",
    "    def train(self, train_dataloader, valid_dataloader, max_epoch):\n",
    "        ''' \n",
    "        :param train_dataloader: train dataloader\n",
    "        :param valid_dataloader: validation dataloader\n",
    "        :param max_epoch: steps per epoch\n",
    "        '''\n",
    "        for epoch in range(0, max_epoch):\n",
    "            train_dataloader.dataset.set_epoch(epoch) # Update epoch for dataset\n",
    "            ################## train #####################\n",
    "            model.train()  # set training flag\n",
    "            for (cur_step,(data, label)) in enumerate(train_dataloader):\n",
    "                data[0] = data[0].to(device)\n",
    "                label[0] = label[0].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                # loss_value = loss_fn(output, label)\n",
    "                loss_value = model.loss(output, label) # use model.loss\n",
    "                loss_value.backward()       \n",
    "                if cur_step%print_interval == 0:\n",
    "                    # batch_acc = accuracy(output,label)\n",
    "                    batch_acc = accuracy(output.backbone_output,label) # use output.backbone_output\n",
    "                    dt = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # date time\n",
    "                    # print(\"[{}] epoch {} step {} : training batch loss {:.4f}, training batch acc {:.4f}\".format(\n",
    "                    #   dt, epoch, cur_step, loss_value.item(), batch_acc.item()))\n",
    "                    print(\"[{}] epoch {} step {} : training batch loss {:.4f}, training batch acc {:.4f}\".format(\n",
    "                      dt, epoch, cur_step, loss_value.backbone_loss.item(), batch_acc.item())) # use loss_value.backbone_loss\n",
    "                self._optimizer.step()\n",
    "            self._scheduler.step()\n",
    "            ################## evaluate ######################\n",
    "            self.evaluate(model, valid_dataloader, epoch)\n",
    "            \n",
    "    def evaluate(self, model, valid_dataloader, epoch):\n",
    "        with torch.no_grad():\n",
    "            model.eval()  \n",
    "            backbone = model.backbone # use backbone in evaluation\n",
    "            loss_cum = 0.0\n",
    "            sample_num = 0\n",
    "            acc_cum = 0.0\n",
    "            for (cur_step,(data, label)) in enumerate(valid_dataloader):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                # output = model(data)\n",
    "                output = backbone(data)[0] # use backbone in evaluation and backbone has multi output\n",
    "                batch_size = data.size(0)\n",
    "                sample_num += batch_size\n",
    "                loss_cum += loss_fn(output, label).item() * batch_size\n",
    "                acc_cum += accuracy(output, label).item() * batch_size\n",
    "            dt = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # date time\n",
    "            if sample_num > 0:\n",
    "                loss_value = loss_cum/sample_num\n",
    "                acc_value = acc_cum/sample_num\n",
    "            else:\n",
    "                loss_value = 0.0\n",
    "                acc_value = 0.0\n",
    "\n",
    "            print(\"[{}] epoch {} : evaluation loss {:.4f}, evaluation acc {:.4f}\".format(\n",
    "                dt, epoch, loss_value, acc_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7615c",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bae383b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-15 07:15:23] epoch 0 step 0 : training batch loss 4.6799, training batch acc 0.0234\n",
      "[2022-11-15 07:15:24] epoch 0 step 10 : training batch loss 4.6780, training batch acc 0.0000\n",
      "[2022-11-15 07:15:25] epoch 0 step 20 : training batch loss 4.5872, training batch acc 0.0078\n",
      "[2022-11-15 07:15:26] epoch 0 step 30 : training batch loss 4.5761, training batch acc 0.0156\n",
      "[2022-11-15 07:15:27] epoch 0 step 40 : training batch loss 4.5971, training batch acc 0.0156\n",
      "[2022-11-15 07:15:27] epoch 0 step 50 : training batch loss 4.5667, training batch acc 0.0156\n",
      "[2022-11-15 07:15:28] epoch 0 step 60 : training batch loss 4.5225, training batch acc 0.0312\n",
      "[2022-11-15 07:15:29] epoch 0 step 70 : training batch loss 4.5195, training batch acc 0.0156\n",
      "[2022-11-15 07:15:30] epoch 0 step 80 : training batch loss 4.5442, training batch acc 0.0391\n",
      "[2022-11-15 07:15:31] epoch 0 step 90 : training batch loss 4.4777, training batch acc 0.0469\n",
      "[2022-11-15 07:15:31] epoch 0 step 100 : training batch loss 4.5422, training batch acc 0.0156\n",
      "[2022-11-15 07:15:32] epoch 0 step 110 : training batch loss 4.4815, training batch acc 0.0312\n",
      "[2022-11-15 07:15:33] epoch 0 step 120 : training batch loss 4.5121, training batch acc 0.0234\n",
      "[2022-11-15 07:15:34] epoch 0 step 130 : training batch loss 4.5219, training batch acc 0.0312\n",
      "[2022-11-15 07:15:34] epoch 0 step 140 : training batch loss 4.4957, training batch acc 0.0234\n",
      "[2022-11-15 07:15:35] epoch 0 step 150 : training batch loss 4.5235, training batch acc 0.0234\n",
      "[2022-11-15 07:15:36] epoch 0 step 160 : training batch loss 4.4479, training batch acc 0.0391\n",
      "[2022-11-15 07:15:36] epoch 0 step 170 : training batch loss 4.4618, training batch acc 0.0469\n",
      "[2022-11-15 07:15:37] epoch 0 step 180 : training batch loss 4.4694, training batch acc 0.0234\n",
      "[2022-11-15 07:15:38] epoch 0 step 190 : training batch loss 4.5019, training batch acc 0.0312\n",
      "[2022-11-15 07:15:39] epoch 0 step 200 : training batch loss 4.4666, training batch acc 0.0391\n",
      "[2022-11-15 07:15:39] epoch 0 step 210 : training batch loss 4.4771, training batch acc 0.0312\n",
      "[2022-11-15 07:15:40] epoch 0 step 220 : training batch loss 4.5340, training batch acc 0.0391\n",
      "[2022-11-15 07:15:41] epoch 0 step 230 : training batch loss 4.4467, training batch acc 0.0391\n",
      "[2022-11-15 07:15:42] epoch 0 step 240 : training batch loss 4.4779, training batch acc 0.0391\n",
      "[2022-11-15 07:15:42] epoch 0 step 250 : training batch loss 4.4785, training batch acc 0.0391\n",
      "[2022-11-15 07:15:43] epoch 0 step 260 : training batch loss 4.4051, training batch acc 0.0703\n",
      "[2022-11-15 07:15:44] epoch 0 step 270 : training batch loss 4.3947, training batch acc 0.0312\n",
      "[2022-11-15 07:15:44] epoch 0 step 280 : training batch loss 4.4758, training batch acc 0.0312\n",
      "[2022-11-15 07:15:45] epoch 0 step 290 : training batch loss 4.3961, training batch acc 0.0234\n",
      "[2022-11-15 07:15:46] epoch 0 step 300 : training batch loss 4.4428, training batch acc 0.0234\n",
      "[2022-11-15 07:15:47] epoch 0 step 310 : training batch loss 4.4133, training batch acc 0.0312\n",
      "[2022-11-15 07:15:47] epoch 0 step 320 : training batch loss 4.4856, training batch acc 0.0000\n",
      "[2022-11-15 07:15:48] epoch 0 step 330 : training batch loss 4.3920, training batch acc 0.0312\n",
      "[2022-11-15 07:15:49] epoch 0 step 340 : training batch loss 4.4689, training batch acc 0.0234\n",
      "[2022-11-15 07:15:50] epoch 0 step 350 : training batch loss 4.5120, training batch acc 0.0000\n",
      "[2022-11-15 07:15:50] epoch 0 step 360 : training batch loss 4.3992, training batch acc 0.0156\n",
      "[2022-11-15 07:15:51] epoch 0 step 370 : training batch loss 4.4291, training batch acc 0.0391\n",
      "[2022-11-15 07:15:52] epoch 0 step 380 : training batch loss 4.4367, training batch acc 0.0312\n",
      "[2022-11-15 07:15:53] epoch 0 step 390 : training batch loss 4.4171, training batch acc 0.0000\n",
      "[2022-11-15 07:15:56] epoch 0 : evaluation loss 4.4339, evaluation acc 0.0324\n",
      "CPU times: user 24min 39s, sys: 24.8 s, total: 25min 4s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(model, optimizer, scheduler)\n",
    "trainer.train(train_loader,validate_loader,max_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
