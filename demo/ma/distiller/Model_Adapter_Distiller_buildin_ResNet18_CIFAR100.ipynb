{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c7ec59",
   "metadata": {},
   "source": [
    "# Model Adapter Distiller Build-in DEMO\n",
    "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
    "\n",
    "This demo mainly introduces the usage of Distiller. Take image classification as an example, it shows how to integrate distiller  from VIT to ResNet18 on CIFAR100 dataset. This is a build-in usage, you can find customized detailed demo at [here](./Model_Adapter_Distiller_customized_ResNet18_CIFAR100.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c721aa",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* [Model Adapter Distiller Overview](#Model-Adapter-Distller-Overview)\n",
    "* [Getting Started](#Getting-Started)\n",
    "    * [Environment Setup](#Environment-Setup)\n",
    "    * [Launch Training on baseline](#Launch-Training-on-baseline)\n",
    "    * [Launch Training with Distiller](#Launch-Training-with-Distiller)\n",
    "* [Performance](#Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462e304",
   "metadata": {},
   "source": [
    "## Model Adapter Distiller Overview\n",
    "Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "\n",
    "<img src=\"../imgs/distiller.png\" width=\"60%\">\n",
    "<center>Model Adapter Distiller Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdf893",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cbf83",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "1. prepare code\n",
    "    ``` bash\n",
    "    git clone https://github.com/intel/e2eAIOK.git\n",
    "    cd e2eAIOK\n",
    "    git submodule update --init –recursive\n",
    "    ```\n",
    "2. build docker image\n",
    "   proxy\n",
    "   ``` bash\n",
    "   python3 scripts/start_e2eaiok_docker.py -b pytorch112 --dataset_path ${dataset_path} -w ${host0} ${host1} ${host2} ${host3} --proxy  \"http://addr:ip\"\n",
    "   ```\n",
    "3. run docker\n",
    "   ``` bash\n",
    "   sshpass -p docker ssh ${host0} -p 12347\n",
    "   ```\n",
    "4. Run in conda and set up e2eAIOK\n",
    "   ```bash\n",
    "   conda activate pytorch-1.12.0\n",
    "   python setup.py sdist && pip install dist/e2eAIOK-*.*.*.tar.gz\n",
    "   ```\n",
    "5. Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2877927",
   "metadata": {},
   "source": [
    "## Launch training on baseline\n",
    "First we train a vanilla ResNet18 on CIFAR100.\n",
    "\n",
    "### Configuration\n",
    "Create a configuration for ResNet18 with CIFAR100.\n",
    "```yaml\n",
    "# basic experiment setting\n",
    "experiment:\n",
    "    project: \"demo\"\n",
    "    tag: \"cifar100_res18\"\n",
    "output_dir: \"/home/vmagent/app/data/model\"\n",
    "\n",
    "# dataset and model setting\n",
    "model_type: \"resnet18\"\n",
    "data_set: \"cifar100\"\n",
    "data_path:  \"/home/vmagent/app/data/dataset/cifar\"\n",
    "num_workers: 0\n",
    "input_size: 224\n",
    "\n",
    "# training setting\n",
    "train_epochs: 1\n",
    "optimizer: \"SGD\"\n",
    "learning_rate: 0.1\n",
    "weight_decay: 0.0001\n",
    "momentum: 0.9\n",
    "\n",
    "lr_scheduler: \"ReduceLROnPlateau\"\n",
    "lr_scheduler_config:\n",
    "    decay_rate: 0.2\n",
    "    decay_patience: 10 # for ReduceLROnPlateau\n",
    "  \n",
    "early_stop: \"EarlyStopping\"\n",
    "early_stop_config:\n",
    "    tolerance_epoch: 15\n",
    "\n",
    "```\n",
    "You can also find this configuration at [here](../../../conf/ma/demo/baseline/cifar100_res18_demo.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad9b98",
   "metadata": {},
   "source": [
    "### Launch training\n",
    "**Training resnet18 on CIFAR100 from scratch (train one epoch for example):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3262cd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "configurations:\n",
      "{'weight_decay': 0.0001, 'warmup_scheduler_epoch': 0, 'device': 'cpu', 'dist_backend': 'gloo', 'train_epochs': 1, 'pin_mem': False, 'eval_epochs': 1, 'early_stop_config': {'delta': 0.0001, 'is_max': True, 'tolerance_epoch': 15}, 'finetuner': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'pretrained_num_classes': 10, 'finetuned_lr': 0.01, 'frozen': False}, 'learning_rate': 0.1, 'momentum': 0.9, 'seed': 0, 'enable_ipex': False, 'start_epoch': 0, 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res18_resnet18_cifar100', 'warmup_scheduler': '', 'train_batch_size': 128, 'output_dir': '/home/vmagent/app/data/model', 'drop_last': False, 'eval_metric': 'accuracy', 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'test_transform': 'default', 'metric_threshold': 100.0, 'pretrain': '', 'criterion': 'CrossEntropyLoss', 'eval_batch_size': 128, 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/baseline/cifar100_res18/profile/profile_resnet18_cifar100_1675759008'}, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'eval_step': 10, 'optimizer': 'SGD', 'num_workers': 4, 'lr_scheduler_config': {'T_max': 0, 'decay_stages': [], 'decay_patience': 10, 'decay_rate': 0.2}, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'train_transform': 'default', 'profiler': False, 'model_save_interval': 40, 'data_set': 'cifar100', 'log_interval_step': 10, 'early_stop': 'EarlyStopping', 'experiment': {'tag': 'cifar100_res18', 'project': 'baseline', 'strategy': ''}, 'kd': {'temperature': 4}, 'model_type': 'resnet18', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'lr_scheduler': 'ReduceLROnPlateau', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'input_size': 112, 'initial_pretrain': ''}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model params:  11227812\n",
      "Epoch [0] learning rate: [0.1]\n",
      "[2023-02-07 08:36:52] rank(0) epoch(0) step (0/391) Train: loss = 5.2861;\taccuracy = 0.7812\n",
      "[2023-02-07 08:37:00] rank(0) epoch(0) step (10/391) Train: loss = 5.3960;\taccuracy = 4.6875\n",
      "[2023-02-07 08:37:05] rank(0) epoch(0) step (20/391) Train: loss = 5.4470;\taccuracy = 0.7812\n",
      "[2023-02-07 08:37:09] rank(0) epoch(0) step (30/391) Train: loss = 4.9619;\taccuracy = 3.9062\n",
      "[2023-02-07 08:37:13] rank(0) epoch(0) step (40/391) Train: loss = 5.0229;\taccuracy = 6.2500\n",
      "[2023-02-07 08:37:17] rank(0) epoch(0) step (50/391) Train: loss = 4.2873;\taccuracy = 4.6875\n",
      "[2023-02-07 08:37:22] rank(0) epoch(0) step (60/391) Train: loss = 4.0606;\taccuracy = 7.0312\n",
      "[2023-02-07 08:37:26] rank(0) epoch(0) step (70/391) Train: loss = 4.4454;\taccuracy = 5.4688\n",
      "[2023-02-07 08:37:30] rank(0) epoch(0) step (80/391) Train: loss = 4.1087;\taccuracy = 4.6875\n",
      "[2023-02-07 08:37:34] rank(0) epoch(0) step (90/391) Train: loss = 3.9366;\taccuracy = 7.8125\n",
      "[2023-02-07 08:37:39] rank(0) epoch(0) step (100/391) Train: loss = 4.2191;\taccuracy = 4.6875\n",
      "[2023-02-07 08:37:46] rank(0) epoch(0) step (110/391) Train: loss = 4.1708;\taccuracy = 4.6875\n",
      "[2023-02-07 08:37:50] rank(0) epoch(0) step (120/391) Train: loss = 4.1467;\taccuracy = 8.5938\n",
      "[2023-02-07 08:37:54] rank(0) epoch(0) step (130/391) Train: loss = 3.8159;\taccuracy = 6.2500\n",
      "[2023-02-07 08:37:58] rank(0) epoch(0) step (140/391) Train: loss = 3.9031;\taccuracy = 10.1562\n",
      "[2023-02-07 08:38:03] rank(0) epoch(0) step (150/391) Train: loss = 3.7629;\taccuracy = 12.5000\n",
      "[2023-02-07 08:38:07] rank(0) epoch(0) step (160/391) Train: loss = 3.9277;\taccuracy = 7.0312\n",
      "[2023-02-07 08:38:11] rank(0) epoch(0) step (170/391) Train: loss = 3.9020;\taccuracy = 14.0625\n",
      "[2023-02-07 08:38:15] rank(0) epoch(0) step (180/391) Train: loss = 3.8196;\taccuracy = 15.6250\n",
      "[2023-02-07 08:38:20] rank(0) epoch(0) step (190/391) Train: loss = 3.7841;\taccuracy = 13.2812\n",
      "[2023-02-07 08:38:24] rank(0) epoch(0) step (200/391) Train: loss = 3.7794;\taccuracy = 9.3750\n",
      "[2023-02-07 08:38:31] rank(0) epoch(0) step (210/391) Train: loss = 3.6708;\taccuracy = 11.7188\n",
      "[2023-02-07 08:38:36] rank(0) epoch(0) step (220/391) Train: loss = 3.8479;\taccuracy = 7.8125\n",
      "[2023-02-07 08:38:40] rank(0) epoch(0) step (230/391) Train: loss = 3.7131;\taccuracy = 11.7188\n",
      "[2023-02-07 08:38:44] rank(0) epoch(0) step (240/391) Train: loss = 3.6719;\taccuracy = 8.5938\n",
      "[2023-02-07 08:38:49] rank(0) epoch(0) step (250/391) Train: loss = 3.7307;\taccuracy = 11.7188\n",
      "[2023-02-07 08:38:53] rank(0) epoch(0) step (260/391) Train: loss = 3.7311;\taccuracy = 14.8438\n",
      "[2023-02-07 08:38:57] rank(0) epoch(0) step (270/391) Train: loss = 3.8440;\taccuracy = 4.6875\n",
      "[2023-02-07 08:39:01] rank(0) epoch(0) step (280/391) Train: loss = 3.6683;\taccuracy = 10.9375\n",
      "[2023-02-07 08:39:06] rank(0) epoch(0) step (290/391) Train: loss = 3.6439;\taccuracy = 12.5000\n",
      "[2023-02-07 08:39:10] rank(0) epoch(0) step (300/391) Train: loss = 3.7166;\taccuracy = 10.1562\n",
      "[2023-02-07 08:39:17] rank(0) epoch(0) step (310/391) Train: loss = 3.7415;\taccuracy = 12.5000\n",
      "[2023-02-07 08:39:22] rank(0) epoch(0) step (320/391) Train: loss = 3.5705;\taccuracy = 15.6250\n",
      "[2023-02-07 08:39:27] rank(0) epoch(0) step (330/391) Train: loss = 3.2945;\taccuracy = 18.7500\n",
      "[2023-02-07 08:39:31] rank(0) epoch(0) step (340/391) Train: loss = 3.7407;\taccuracy = 14.8438\n",
      "[2023-02-07 08:39:35] rank(0) epoch(0) step (350/391) Train: loss = 3.5115;\taccuracy = 18.7500\n",
      "[2023-02-07 08:39:39] rank(0) epoch(0) step (360/391) Train: loss = 3.3647;\taccuracy = 15.6250\n",
      "[2023-02-07 08:39:43] rank(0) epoch(0) step (370/391) Train: loss = 3.5443;\taccuracy = 11.7188\n",
      "[2023-02-07 08:39:48] rank(0) epoch(0) step (380/391) Train: loss = 3.7552;\taccuracy = 11.7188\n",
      "[2023-02-07 08:39:51] rank(0) epoch(0) step (390/391) Train: loss = 3.3477;\taccuracy = 20.0000\n",
      "2023-02-07 08:39:55 0/391\n",
      "2023-02-07 08:39:56 10/391\n",
      "2023-02-07 08:39:58 20/391\n",
      "2023-02-07 08:39:59 30/391\n",
      "2023-02-07 08:40:00 40/391\n",
      "2023-02-07 08:40:01 50/391\n",
      "2023-02-07 08:40:03 60/391\n",
      "2023-02-07 08:40:04 70/391\n",
      "[2023-02-07 08:40:05] rank(0) epoch(0) Validation: accuracy = 16.3700;\tloss = 3.4843\n",
      "Best Epoch: 0, accuracy: 16.3700008392334\n",
      "Epoch 0 took 197.68244624137878 seconds\n",
      "Total seconds:197.683502\n",
      "Totally take 200.56229305267334 seconds\n"
     ]
    }
   ],
   "source": [
    "! python /home/vmagent/app/e2eaiok/e2eAIOK/ModelAdapter/main.py --cfg /home/vmagent/app/e2eaiok/conf/ma/demo/baseline/cifar100_res18_demo.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec69744",
   "metadata": {},
   "source": [
    "## Launch Training with Distiller\n",
    "Then we train ResNet18 on CIFAR100 with Distiller to show the performance imrpovement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc056f90",
   "metadata": {},
   "source": [
    "### Prepare teacher model or logits\n",
    "To use distiller, we need to prepare teacher model to guide the training. Directly download teacher model VIT pretrained on CIFAR100 from [here](https://huggingface.co/edumunozsala/vit_base-224-in21k-ft-cifar100), and put it at `${pretrain}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72d83b",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Create a configuration for Distiller with ResNet18 with CIFAR100.\n",
    "\n",
    "```yaml\n",
    "# basic experiment setting\n",
    "experiment:\n",
    "  project: \"demo\"\n",
    "  tag: \"cifar100_kd_vit_res18\"\n",
    "  strategy: \"OnlyDistillationStrategy\"\n",
    "output_dir: \"/home/vmagent/app/data/model\"\n",
    "\n",
    "### dataset and model setting\n",
    "data_set: \"cifar100\"\n",
    "data_path:  \"/home/vmagent/app/data/dataset/cifar\"\n",
    "num_workers: 4\n",
    "train_transform: \"vit\"\n",
    "test_transform: \"vit\"\n",
    "input_size: 224\n",
    "\n",
    "model_type: \"resnet18\"\n",
    "\n",
    "# distiller setting\n",
    "loss_weight:\n",
    "    backbone: 0.1\n",
    "    distiller: 0.9\n",
    "\n",
    "## distiller\n",
    "distiller:\n",
    "    type: \"kd\"\n",
    "    teacher: \n",
    "        type: \"huggingface_vit_base_224_in21k_ft_cifar100\"\n",
    "        initial_pretrain: True\n",
    "\n",
    "## training setting\n",
    "train_epochs: 1\n",
    "optimizer: \"SGD\"\n",
    "learning_rate: 0.1\n",
    "weight_decay: 0.0001\n",
    "momentum: 0.9\n",
    "\n",
    "lr_scheduler: \"ReduceLROnPlateau\"\n",
    "lr_scheduler_config:\n",
    "    decay_rate: 0.2\n",
    "    decay_patience: 10 # for ReduceLROnPlateau\n",
    "\n",
    "early_stop: \"EarlyStopping\"\n",
    "early_stop_config:\n",
    "    tolerance_epoch: 15\n",
    "```\n",
    "\n",
    "You can also find this configuration at [here](../../../conf/ma/demo/distiller/cifar100_kd_vit_res18_demo.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35068e74",
   "metadata": {},
   "source": [
    "### Launch Training with Distiller\n",
    "**Training resnet18 on CIFAR100 with Distiller (train one epoch for example):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae9f14",
   "metadata": {},
   "source": [
    "This may take some time, just go to have a break~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6779f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "configurations:\n",
      "{'model_save_interval': 40, 'output_dir': '/home/vmagent/app/data/model', 'data_set': 'cifar100', 'optimizer': 'SGD', 'input_size': 112, 'learning_rate': 0.1, 'model_type': 'resnet18', 'loss_weight': {'distiller': 0.9, 'adapter': 0.0, 'backbone': 0.1}, 'lr_scheduler_config': {'T_max': 0, 'decay_rate': 0.2, 'decay_patience': 10, 'decay_stages': []}, 'profiler': False, 'train_epochs': 1, 'early_stop': 'EarlyStopping', 'criterion': 'CrossEntropyLoss', 'experiment': {'tag': 'cifar100_kd_res50_res18', 'strategy': 'OnlyDistillationStrategy', 'project': 'demo'}, 'warmup_scheduler': '', 'initial_pretrain': '', 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'pretrain': '', 'distiller': {'teacher': {'type': 'resnet50', 'initial_pretrain': '', 'frozen': True, 'pretrain': '/home/vmagent/app/data/model/demo/baseline/cifar100_res50PretrainI21k/cifar100_res50_pretrain_imagenet21k.pth'}, 'check_logits': False, 'use_saved_logits': True, 'type': 'kd', 'logits_path': '/home/vmagent/app/data/model/demo/distiller/cifar100_kd_res50PretrainI21k/logits', 'logits_topk': 0, 'save_logits': False, 'save_logits_start_epoch': 0}, 'eval_step': 10, 'drop_last': False, 'metric_threshold': 100.0, 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_kd_res50_res18_resnet18_OnlyDistillationStrategy_cifar100', 'warmup_scheduler_epoch': 0, 'train_transform': 'default', 'eval_metric': 'accuracy', 'log_interval_step': 10, 'seed': 0, 'lr_scheduler': 'ReduceLROnPlateau', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'start_epoch': 0, 'num_workers': 4, 'pin_mem': False, 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/demo/cifar100_kd_res50_res18/profile/profile_resnet18_OnlyDistillationStrategy_cifar100_1675759244'}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'train_batch_size': 128, 'dist_backend': 'gloo', 'kd': {'temperature': 4}, 'eval_epochs': 1, 'eval_batch_size': 128, 'enable_ipex': False, 'test_transform': 'default', 'momentum': 0.9, 'finetuner': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'pretrained_num_classes': 10, 'finetuned_lr': 0.01, 'frozen': False}, 'weight_decay': 0.0001, 'device': 'cpu'}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model params:  11227812\n",
      "Epoch [0] learning rate: [0.1]\n",
      "[2023-02-07 08:40:49] rank(0) epoch(0) step (0/391) Train: total_loss = 8.1885;\tbackbone_loss = 5.3516;\tdistiller_loss = 8.5037;\taccuracy = 0.7812\n",
      "[2023-02-07 08:41:02] rank(0) epoch(0) step (10/391) Train: total_loss = 8.2124;\tbackbone_loss = 5.0974;\tdistiller_loss = 8.5586;\taccuracy = 1.5625\n",
      "[2023-02-07 08:41:08] rank(0) epoch(0) step (20/391) Train: total_loss = 7.7583;\tbackbone_loss = 4.6938;\tdistiller_loss = 8.0988;\taccuracy = 3.9062\n",
      "[2023-02-07 08:41:16] rank(0) epoch(0) step (30/391) Train: total_loss = 7.3798;\tbackbone_loss = 4.6016;\tdistiller_loss = 7.6885;\taccuracy = 4.6875\n",
      "[2023-02-07 08:41:24] rank(0) epoch(0) step (40/391) Train: total_loss = 7.0233;\tbackbone_loss = 4.4144;\tdistiller_loss = 7.3132;\taccuracy = 7.8125\n",
      "[2023-02-07 08:41:32] rank(0) epoch(0) step (50/391) Train: total_loss = 6.6239;\tbackbone_loss = 4.3423;\tdistiller_loss = 6.8774;\taccuracy = 4.6875\n",
      "[2023-02-07 08:41:38] rank(0) epoch(0) step (60/391) Train: total_loss = 6.3073;\tbackbone_loss = 3.9786;\tdistiller_loss = 6.5660;\taccuracy = 7.8125\n",
      "[2023-02-07 08:41:45] rank(0) epoch(0) step (70/391) Train: total_loss = 6.2883;\tbackbone_loss = 3.9098;\tdistiller_loss = 6.5526;\taccuracy = 6.2500\n",
      "[2023-02-07 08:41:53] rank(0) epoch(0) step (80/391) Train: total_loss = 6.7961;\tbackbone_loss = 4.2350;\tdistiller_loss = 7.0807;\taccuracy = 10.1562\n",
      "[2023-02-07 08:41:59] rank(0) epoch(0) step (90/391) Train: total_loss = 6.6584;\tbackbone_loss = 3.9942;\tdistiller_loss = 6.9545;\taccuracy = 10.1562\n",
      "[2023-02-07 08:42:07] rank(0) epoch(0) step (100/391) Train: total_loss = 6.3157;\tbackbone_loss = 4.0304;\tdistiller_loss = 6.5697;\taccuracy = 7.0312\n",
      "[2023-02-07 08:42:21] rank(0) epoch(0) step (110/391) Train: total_loss = 6.3195;\tbackbone_loss = 3.7142;\tdistiller_loss = 6.6089;\taccuracy = 10.1562\n",
      "[2023-02-07 08:42:31] rank(0) epoch(0) step (120/391) Train: total_loss = 6.2158;\tbackbone_loss = 3.6808;\tdistiller_loss = 6.4975;\taccuracy = 16.4062\n",
      "[2023-02-07 08:42:40] rank(0) epoch(0) step (130/391) Train: total_loss = 6.1728;\tbackbone_loss = 3.6710;\tdistiller_loss = 6.4507;\taccuracy = 11.7188\n",
      "[2023-02-07 08:42:47] rank(0) epoch(0) step (140/391) Train: total_loss = 6.2329;\tbackbone_loss = 3.8242;\tdistiller_loss = 6.5005;\taccuracy = 10.9375\n",
      "[2023-02-07 08:42:56] rank(0) epoch(0) step (150/391) Train: total_loss = 6.2125;\tbackbone_loss = 3.8639;\tdistiller_loss = 6.4734;\taccuracy = 11.7188\n",
      "[2023-02-07 08:43:04] rank(0) epoch(0) step (160/391) Train: total_loss = 5.8850;\tbackbone_loss = 3.9415;\tdistiller_loss = 6.1010;\taccuracy = 9.3750\n",
      "[2023-02-07 08:43:13] rank(0) epoch(0) step (170/391) Train: total_loss = 5.9810;\tbackbone_loss = 3.8014;\tdistiller_loss = 6.2231;\taccuracy = 11.7188\n",
      "[2023-02-07 08:43:21] rank(0) epoch(0) step (180/391) Train: total_loss = 5.6300;\tbackbone_loss = 3.5932;\tdistiller_loss = 5.8563;\taccuracy = 11.7188\n",
      "[2023-02-07 08:43:29] rank(0) epoch(0) step (190/391) Train: total_loss = 6.1934;\tbackbone_loss = 3.9206;\tdistiller_loss = 6.4459;\taccuracy = 7.0312\n",
      "[2023-02-07 08:43:37] rank(0) epoch(0) step (200/391) Train: total_loss = 5.8905;\tbackbone_loss = 3.4782;\tdistiller_loss = 6.1585;\taccuracy = 22.6562\n",
      "[2023-02-07 08:43:51] rank(0) epoch(0) step (210/391) Train: total_loss = 5.6806;\tbackbone_loss = 3.7180;\tdistiller_loss = 5.8987;\taccuracy = 14.0625\n",
      "[2023-02-07 08:43:57] rank(0) epoch(0) step (220/391) Train: total_loss = 5.8826;\tbackbone_loss = 3.5233;\tdistiller_loss = 6.1447;\taccuracy = 14.0625\n",
      "[2023-02-07 08:44:01] rank(0) epoch(0) step (230/391) Train: total_loss = 6.0930;\tbackbone_loss = 3.4578;\tdistiller_loss = 6.3858;\taccuracy = 19.5312\n",
      "[2023-02-07 08:44:05] rank(0) epoch(0) step (240/391) Train: total_loss = 6.1756;\tbackbone_loss = 3.4828;\tdistiller_loss = 6.4748;\taccuracy = 17.1875\n",
      "[2023-02-07 08:44:09] rank(0) epoch(0) step (250/391) Train: total_loss = 5.4331;\tbackbone_loss = 3.1981;\tdistiller_loss = 5.6814;\taccuracy = 19.5312\n",
      "[2023-02-07 08:44:12] rank(0) epoch(0) step (260/391) Train: total_loss = 5.7343;\tbackbone_loss = 3.7956;\tdistiller_loss = 5.9497;\taccuracy = 16.4062\n",
      "[2023-02-07 08:44:16] rank(0) epoch(0) step (270/391) Train: total_loss = 5.7587;\tbackbone_loss = 3.7067;\tdistiller_loss = 5.9867;\taccuracy = 14.8438\n",
      "[2023-02-07 08:44:19] rank(0) epoch(0) step (280/391) Train: total_loss = 5.4625;\tbackbone_loss = 3.6463;\tdistiller_loss = 5.6643;\taccuracy = 14.0625\n",
      "[2023-02-07 08:44:23] rank(0) epoch(0) step (290/391) Train: total_loss = 5.4235;\tbackbone_loss = 3.5627;\tdistiller_loss = 5.6302;\taccuracy = 17.9688\n",
      "[2023-02-07 08:44:26] rank(0) epoch(0) step (300/391) Train: total_loss = 5.7582;\tbackbone_loss = 3.8674;\tdistiller_loss = 5.9683;\taccuracy = 14.0625\n",
      "[2023-02-07 08:44:35] rank(0) epoch(0) step (310/391) Train: total_loss = 5.6317;\tbackbone_loss = 3.2023;\tdistiller_loss = 5.9017;\taccuracy = 23.4375\n",
      "[2023-02-07 08:44:39] rank(0) epoch(0) step (320/391) Train: total_loss = 5.1355;\tbackbone_loss = 3.4310;\tdistiller_loss = 5.3249;\taccuracy = 18.7500\n",
      "[2023-02-07 08:44:43] rank(0) epoch(0) step (330/391) Train: total_loss = 5.5356;\tbackbone_loss = 3.3043;\tdistiller_loss = 5.7835;\taccuracy = 25.0000\n",
      "[2023-02-07 08:44:46] rank(0) epoch(0) step (340/391) Train: total_loss = 5.2567;\tbackbone_loss = 3.5325;\tdistiller_loss = 5.4483;\taccuracy = 16.4062\n",
      "[2023-02-07 08:44:50] rank(0) epoch(0) step (350/391) Train: total_loss = 5.7273;\tbackbone_loss = 3.6882;\tdistiller_loss = 5.9538;\taccuracy = 11.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-07 08:44:53] rank(0) epoch(0) step (360/391) Train: total_loss = 5.4664;\tbackbone_loss = 3.4247;\tdistiller_loss = 5.6932;\taccuracy = 21.8750\n",
      "[2023-02-07 08:44:57] rank(0) epoch(0) step (370/391) Train: total_loss = 5.1615;\tbackbone_loss = 3.1988;\tdistiller_loss = 5.3796;\taccuracy = 23.4375\n",
      "[2023-02-07 08:45:00] rank(0) epoch(0) step (380/391) Train: total_loss = 5.8639;\tbackbone_loss = 3.5364;\tdistiller_loss = 6.1225;\taccuracy = 17.9688\n",
      "[2023-02-07 08:45:04] rank(0) epoch(0) step (390/391) Train: total_loss = 5.2738;\tbackbone_loss = 3.4408;\tdistiller_loss = 5.4774;\taccuracy = 22.5000\n",
      "2023-02-07 08:45:10 0/391\n",
      "2023-02-07 08:45:11 10/391\n",
      "2023-02-07 08:45:12 20/391\n",
      "2023-02-07 08:45:13 30/391\n",
      "2023-02-07 08:45:14 40/391\n",
      "2023-02-07 08:45:15 50/391\n",
      "2023-02-07 08:45:16 60/391\n",
      "2023-02-07 08:45:17 70/391\n",
      "[2023-02-07 08:45:18] rank(0) epoch(0) Validation: accuracy = 19.1100;\tloss = 3.6392\n",
      "Best Epoch: 0, accuracy: 19.110000610351562\n",
      "Epoch 0 took 277.65441823005676 seconds\n",
      "Total seconds:277.656086\n",
      "Totally take 281.1705300807953 seconds\n"
     ]
    }
   ],
   "source": [
    "! python /home/vmagent/app/e2eaiok/e2eAIOK/ModelAdapter/main.py --cfg /home/vmagent/app/e2eaiok/conf/ma/demo/distiller/cifar100_kd_vit_res18_demo.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
