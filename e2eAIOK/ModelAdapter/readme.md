# AIOK Model Adaptor: Enhance AI Pipeline with Knowledge Transfer
## INTRODUCTION 
### Problem Statement

With the development of technology, more and more advanced models have been released. For example, GPT-3 model has 175B parameters, and is trained on 500B dataset. These models, while achieving SOTA results, are only available for head players. There are several challenges in applying these huge and complex models. For example, expensive training cost, huge efforts of data labeling, and high hardware resource requirement for deployment. 

### Solution with Model Adaptor of Intel® End-to-End AI Optimization Kit
Some technologies have been proposed to tackle these challenges. For example, pre-training & fine-tuning mechanism can greatly reduce the training cost; knowledge distillation can significantly reduce the hardware resource requirement; besides, domain adaptation can train target model with few-label or even label-free. Many related toolkits have been developed, but they mostly only focus on one of these technologies. We propose a Model Adaptor toolkit, which combines all these technologies with a unified Application Programming Interface (API). 

AIOK Model Adaptor toolkit is a convenient framework can be used to reduce training and inference time,  or data labeling cost by efficiently utilizing public advanced models and datasets from many domains. Its objectives are:(1)Transfer knowledge from pretrained model with the same/different network structure, which greatly speedups training without accuracy regression. (2)Transfer knowledge from source domain data without target label.

### This solution is intended for
This solution is intended for citizen data scientists, enterprise users, independent software vendor and partial of cloud service provider.

## ARCHITECTURE 
### Model Adaptor of Intel® End-to-End AI Optimization Kit
There are three modules in Model Adapter: Finetuner for pretraining & fine-tuning, Distiller for knowledge distillation and Adapter for domain adaption. They all share a unified API and can be easily integrated with existing pipeline with few codes modification. What’s more, Model Adaptor makes additional efforts on CPU optimization of training and inference, both on single-node and distributed modes.

### The key components are

- **Finetuner**: Finetuner is based on pretraining and finetuning technology, it can transfer knowledge from pretrained model to target model with same network structure. Pretrained models usually are generated by pretraining process, which is training specific model on specific dataset and has been performed by DE-NAS, PyTorch, TensorFlow, or HuggingFace. Finetunner retrieves the pretrained model with same network structure, and copy pretrained weights from pretrained model to corresponding layer of target model, instead of random initialization for target mode. With finetunner, we can greatly improve training speed, and usually achieves better performance.

- **Distiller**: Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.

- **Adapter**: Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.

<img src="./doc/overview.png" width="60%">


## Getting Started 

### Installing
    ```bash
    git clone https://github.com/intel/e2eAIOK.git
    git submodule update --init --recursive
    ```

### Build-in Demos
- [Model Adapter Overview](../../demo/ma/Model_Adapter_Summary.ipynb) 
- [Finetuner](../../demo/ma/finetuner/Model_Adapter_Finetuner_buildin_resnet50_CIFAR100.ipynb) - Apply finetuner on Image Classification.
- [Distiller](../../demo/ma/distiller/Model_Adapter_Distiller_buildin_resnet18_CIFAR100.ipynb) - Apply distiller on Image Classificationt.
- [Domain Adapter](../../demo/ma/adapter/Model_Adapter_Domain_Adapter_buildin_Unet_KITS19.ipynb) - Apply domain adapter on Medical Segmentation.
 
### Customized usage

 We provide an unified API for all three components, , which is an unified interface to assign different transfer learning ability to the underlying model and can be easily integrated with existing pipeline with few codes modification.

#### Finetuner

1. Download the pretrained resnet50 from [ImageNet-21K Pretraining for the Masses](https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/resnet50_miil_21k.pth)[4], which is pretrained on Imagenet21k.

2. Create a transferable model with Model Adaptor Unified API:
    ```python
    model = timm.create_model('resnet50', pretrained=False, num_classes=100)

    pretrained_path = './resnet50_miil_21k.pth' # download path
    pretrained_model = timm.create_model('resnet50', pretrained=False, num_classes=11221)
    pretrained_model.load_state_dict(torch.load(pretrained_path,map_location=device)["state_dict"], strict=True)
    finetunner= BasicFinetunner(pretrained_model, is_frozen=False)
    model = make_transferrable_with_finetune(model, loss_fn, finetunner)
    ```
    You can find a complete demo at [finetuner customized demo](../../demo/ma/finetuner/Model_Adapter_Finetuner_customized_resnet50_CIFAR100.ipynb).

#### Distiller

1. Prepare a teacher model, here we select pretrained vit_base-224-in21k-ft-cifar100 is from [HuggingFace](https://huggingface.co/edumunozsala/vit_base-224-in21k-ft-cifar100).

2. Create a transferable model with Model Adaptor Unified API:
   ```python
   model = timm.create_model('resnet50', pretrained=False, num_classes=100)

   from transformers import ViTForImageClassification
   teacher_model = AutoModelForImageClassification.from_pretrained("edumunozsala/vit_base-224-in21k-ft-cifar100")
   distiller= KD(teacher_model)
   loss_fn = torch.nn.CrossEntropyLoss()
   model = make_transferrable_with_knowledge_distillation(model, loss_fn, distiller)
   ```
   
   You can find a complete demo at [distiller customized demo](../../demo/ma/distiller/Model_Adapter_Distiller_customized_resnet18_CIFAR100.ipynb)

**Acceleration with logits saving**
During distillation, teacher forwarding usually takes a lot of time. To accelerate the training procedure, We can save the predicting logits from teacher in advance and reuse it in later student training. Here is the [Logits saving demo](../../demo/ma/distiller/Model_Adapter_Distiller_customized_resnet18_CIFAR100_save_logits.ipynb) and the code for [training with saved logits](../../demo/ma/distiller/Model_Adapter_Distiller_customized_resnet18_CIFAR100_train_with_logits.ipynb)

#### Domain Adapter

1. Create backbone model and discriminator model:
   ```python
   from e2eAIOK.ModelAdapter.backbone.unet.generic_UNet_DA import Generic_UNet_DA
   from e2eAIOK.ModelAdapter.engine_core.adapter.adversarial.DA_Loss import CACDomainAdversarialLoss

   # create backbone
   backbone_kwargs = {...}
   model = Generic_UNet_DA(**backbone_kwargs)

   # create discriminator model
   adv_kwargs = {...}
   adapter = CACDomainAdversarialLoss(**adv_kwargs)
   ```
2. Create a transferable model with Model Adaptor Unified API:
   ```python
   from e2eAIOK.ModelAdapter.engine_core.transferrable_model import TransferStrategy
   transfer_strategy = TransferStrategy.OnlyDomainAdaptionStrategy
   model = make_transferrable_with_domain_adaption(model, adapter, transfer_strategy,...)
   ```
