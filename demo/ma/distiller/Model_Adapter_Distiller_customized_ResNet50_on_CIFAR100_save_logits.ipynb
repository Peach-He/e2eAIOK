{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea0224",
   "metadata": {},
   "source": [
    "# AIOK Model Adapter Distiller Customized DEMO - Save logits\n",
    "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and datasets. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
    "\n",
    "This demo mainly introduces the usage of Distiller saving logits function. Take image classification as an example, it shows how to use distiller to save logits from ResNet50 pretrained model, which will be used to guide the learning of ResNet18 in next [demo](./Model_Adapter_Distiller_customized_ResNet18_CIFAR100_train_with_logits)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb236d1",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* [Model Adapter Distiller Overview](#Model-Adapter-Distller-Overview)\n",
    "* [Environment Setup](#Environment-Setup)\n",
    "* [Save Logits with Distiller](#Save-Logits-with-Distiller)\n",
    "    * [Prepare Data](#Prepare-Data)\n",
    "    * [Create Distiller](#Create-Distiller)\n",
    "    * [Save Logits](#Save-Logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a0a46",
   "metadata": {},
   "source": [
    "# Model Adapter Distiller Overview\n",
    "Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "\n",
    "<img src=\"../imgs/distiller.png\" width=\"60%\">\n",
    "<center>Model Adapter Distiller Structure</center>\n",
    "\n",
    "However, during the distillation process, teacher forwarding usually takes a lot of time. We can use logits saving function in distiller to save predictions from teacher in adavance, then lots of time can be saved during student training. In this notebook, we will show how to use logits saving function, and here we still take ResNet50 on CIFAR100 as an example.\n",
    "\n",
    "To enable saving logits function, we just need to add two steps in original pipeline:\n",
    "- Wrap train_dataset with DataWrapper\n",
    "- Call prepare_logits() in Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d390c",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "1. prepare code\n",
    "    ``` bash\n",
    "    git clone https://github.com/intel/e2eAIOK.git\n",
    "    cd e2eAIOK\n",
    "    git submodule update --init –recursive\n",
    "    ```\n",
    "2. build docker image\n",
    "   ```\n",
    "   cd Dockerfile-ubuntu18.04 && docker build -t e2eaiok-pytorch112 . -f DockerfilePytorch112 && cd .. && yes \n",
    "   ```\n",
    "3. run docker\n",
    "   ``` bash\n",
    "   docker run -it --name model_adapter --shm-size=10g --privileged --network host \\\n",
    "   -v ${dataset_path}:/home/vmagent/app/data  \\\n",
    "   -v `pwd`:/home/vmagent/app/e2eaiok \\\n",
    "   -w /home/vmagent/app/e2eaiok e2eaiok-pytorch112 /bin/bash \n",
    "   ```\n",
    "4. Run in conda and set up e2eAIOK\n",
    "   ```bash\n",
    "   conda activate pytorch-1.12.0\n",
    "   python setup.py sdist && pip install dist/e2eAIOK-*.*.*.tar.gz\n",
    "   ```\n",
    "5. Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ce35d",
   "metadata": {},
   "source": [
    "# Save Logits with Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c2e32",
   "metadata": {},
   "source": [
    "Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f878badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28cbf4",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "### Prepare transformer and dataset\n",
    "For teacher, as pretrained model is trained on large imgage size, scale 32\\*32 to 112\\*112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c016312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343) # mean for 3 channels\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)  # std for 3 channels\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.Resize(112), \n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69896736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_folder='/home/vmagent/app/data/dataset/cifar' # dataset location\n",
    "train_set = datasets.CIFAR100(root=data_folder, train=True, download=True, transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ccda0",
   "metadata": {},
   "source": [
    "### Warp dataset with DataWrapper\n",
    "Warp train dataset with DataWrapper, which helps to save data augmentation information during the forwarding of teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b46a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2eAIOK.ModelAdapter.engine_core.distiller.utils import logits_wrap_dataset\n",
    "logits_path = \"/home/vmagent/app/data/model/demo/distiller/cifar100_kd_res50PretrainI21k/logits_demo\" # path to save the logits\n",
    "train_set = logits_wrap_dataset(train_set, logits_path=logits_path, num_classes=100, save_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210a1e2",
   "metadata": {},
   "source": [
    "### Create dataloader\n",
    "\n",
    "Note: We need to save all the data without any sampling, make sure you have disable \"channel_last\" or \"sampler\" in dataloader, which can avoid data lossing in later logits using process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232dee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=128, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc18f19",
   "metadata": {},
   "source": [
    "## Create Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ac60a",
   "metadata": {},
   "source": [
    "### Create teacher model\n",
    "To use distiller, we need to prepare teacher model to guide the training. Directly download teacher model Resnet50 pretrained on CIFAR100 from [here](), and put it at `${dataset}/model/demo/baseline/cifar100_res50PretrainI21k/cifar100_res50_pretrain_imagenet21k.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b99862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model = \"/home/vmagent/app/data/model/demo/baseline/cifar100_res50PretrainI21k/cifar100_res50_pretrain_imagenet21k.pth\"\n",
    "teacher_model = timm.create_model('resnet50', pretrained=False, num_classes=100)\n",
    "teacher_model.load_state_dict(torch.load(pretrain_model), strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481901ed",
   "metadata": {},
   "source": [
    "### Create Distiller with KD type\n",
    "Here we define a distiller using KD algorithm, and it take a teacher model as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d750e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2eAIOK.ModelAdapter.engine_core.distiller import KD\n",
    "distiller= KD(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f1c55",
   "metadata": {},
   "source": [
    "## Save Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cedb3",
   "metadata": {},
   "source": [
    "Call prepare_logits() of distiller to save the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15548288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-07 08:34:20 save 0/391\n",
      "2023-02-07 08:34:32 save 10/391\n",
      "2023-02-07 08:34:43 save 20/391\n",
      "2023-02-07 08:34:54 save 30/391\n",
      "2023-02-07 08:35:05 save 40/391\n",
      "2023-02-07 08:35:17 save 50/391\n",
      "2023-02-07 08:35:28 save 60/391\n",
      "2023-02-07 08:35:39 save 70/391\n",
      "2023-02-07 08:35:50 save 80/391\n",
      "2023-02-07 08:36:02 save 90/391\n",
      "2023-02-07 08:36:13 save 100/391\n",
      "2023-02-07 08:36:24 save 110/391\n",
      "2023-02-07 08:36:35 save 120/391\n",
      "2023-02-07 08:36:46 save 130/391\n",
      "2023-02-07 08:36:59 save 140/391\n",
      "2023-02-07 08:37:25 save 150/391\n",
      "2023-02-07 08:37:51 save 160/391\n",
      "2023-02-07 08:38:19 save 170/391\n",
      "2023-02-07 08:38:42 save 180/391\n",
      "2023-02-07 08:39:10 save 190/391\n",
      "2023-02-07 08:39:33 save 200/391\n",
      "2023-02-07 08:39:58 save 210/391\n",
      "2023-02-07 08:40:15 save 220/391\n",
      "2023-02-07 08:40:27 save 230/391\n",
      "2023-02-07 08:40:39 save 240/391\n",
      "2023-02-07 08:40:51 save 250/391\n",
      "2023-02-07 08:41:07 save 260/391\n",
      "2023-02-07 08:41:20 save 270/391\n",
      "2023-02-07 08:41:33 save 280/391\n",
      "2023-02-07 08:41:47 save 290/391\n",
      "2023-02-07 08:42:00 save 300/391\n",
      "2023-02-07 08:42:13 save 310/391\n",
      "2023-02-07 08:42:26 save 320/391\n",
      "2023-02-07 08:42:38 save 330/391\n",
      "2023-02-07 08:42:52 save 340/391\n",
      "2023-02-07 08:43:05 save 350/391\n",
      "2023-02-07 08:43:17 save 360/391\n",
      "2023-02-07 08:43:30 save 370/391\n",
      "2023-02-07 08:43:42 save 380/391\n",
      "Save logits over: /home/vmagent/app/data/model/demo/distiller/cifar100_kd_res50PretrainI21k/logits_demo/logits_epoch0\n",
      "2023-02-07 08:43:55 save 390/391\n",
      "Epoch 0 took 576.3161263465881 seconds\n"
     ]
    }
   ],
   "source": [
    "distiller.prepare_logits(train_loader, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
